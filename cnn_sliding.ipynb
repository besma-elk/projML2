{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/inesbahej/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import matplotlib.image as mpimg\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 3 # RGB images\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 2\n",
    "TRAINING_SIZE = 40\n",
    "VALIDATION_SIZE = 5  # Size of the validation set.\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "BATCH_SIZE = 16 # 64\n",
    "NUM_EPOCHS = 50 #5\n",
    "RESTORE_MODEL = False # If True, restore existing model instead of training a new one\n",
    "RECORDING_STEP = 1000\n",
    "IMG_PATCH_SIZE = 1\n",
    "\n",
    "def load_image(infilename):\n",
    "    data = mpimg.imread(infilename)\n",
    "    return data\n",
    "\n",
    "def img_float_to_uint8(img):\n",
    "    rimg = img - numpy.min(img)\n",
    "    rimg = (rimg / numpy.max(rimg) * PIXEL_DEPTH).round().astype(numpy.uint8)\n",
    "    return rimg\n",
    "\n",
    "def concatenate_images(img, gt_img):\n",
    "    nChannels = len(gt_img.shape)\n",
    "    w = gt_img.shape[0]\n",
    "    h = gt_img.shape[1]\n",
    "    if nChannels == 3:\n",
    "        cimg = numpy.concatenate((img, gt_img), axis=1)\n",
    "    else:\n",
    "        gt_img_3c = numpy.zeros((w, h, 3), dtype=numpy.uint8)\n",
    "        gt_img8 = img_float_to_uint8(gt_img)          \n",
    "        gt_img_3c[:,:,0] = gt_img8\n",
    "        gt_img_3c[:,:,1] = gt_img8\n",
    "        gt_img_3c[:,:,2] = gt_img8\n",
    "        img8 = img_float_to_uint8(img)\n",
    "        cimg = numpy.concatenate((img8, gt_img_3c), axis=1)\n",
    "    return cimg\n",
    "\n",
    "\n",
    "def img_crop(im, w, h):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if is_2d:\n",
    "                im_patch = im[j:j+w, i:i+h]\n",
    "            else:\n",
    "                im_patch = im[j:j+w, i:i+h, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches\n",
    "\n",
    "foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "\n",
    "\n",
    "    \n",
    "def extract_labels(filename, num_images):\n",
    "    \"\"\"Extract the labels into a 1-hot matrix [image index, label index].\"\"\"\n",
    "    gt_imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            gt_imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    num_images = len(gt_imgs)\n",
    "    #print(gt_imgs[0].shape)\n",
    "    gt_exp = numpy.expand_dims(gt_imgs, axis=3)\n",
    "    #print(tmp.shape)\n",
    "    IMG_WIDTH = gt_exp[0].shape[0]\n",
    "    IMG_HEIGHT = gt_exp[0].shape[1]\n",
    "    print(IMG_WIDTH, IMG_HEIGHT)\n",
    "    gt_patches = [img_crop(gt_exp[i], IMG_WIDTH, IMG_HEIGHT) for i in range(num_images)]\n",
    "    data = [gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))]\n",
    "    #labels = numpy.asarray([value_to_class(numpy.mean(data[i])) for i in range(len(data))])\n",
    "\n",
    "    # Convert to dense 1-hot representation.\n",
    "    #return labels.astype(numpy.float32)\n",
    "    return numpy.asarray(data)\n",
    "\n",
    "\n",
    "def extract_data(filename, num_images):\n",
    "    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    num_images = len(imgs)\n",
    "    print(imgs[0].shape)\n",
    "    IMG_WIDTH = imgs[0].shape[0]\n",
    "    IMG_HEIGHT = imgs[0].shape[1]\n",
    "    #N_PATCHES_PER_IMAGE = (IMG_WIDTH/IMG_PATCH_SIZE)*(IMG_HEIGHT/IMG_PATCH_SIZE)\n",
    "\n",
    "    img_patches = [img_crop(imgs[i], IMG_WIDTH, IMG_HEIGHT) for i in range(num_images)]\n",
    "    data = [img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))]\n",
    "\n",
    "    return numpy.asarray(data)\n",
    "    #return imgs\n",
    "    \n",
    "    \n",
    "\n",
    "def value_to_class(v):\n",
    "    foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "    df = numpy.sum(v)\n",
    "    if df > foreground_threshold:\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        return [1, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training/images/satImage_001.png\n",
      "Loading training/images/satImage_002.png\n",
      "Loading training/images/satImage_003.png\n",
      "Loading training/images/satImage_004.png\n",
      "Loading training/images/satImage_005.png\n",
      "Loading training/images/satImage_006.png\n",
      "Loading training/images/satImage_007.png\n",
      "Loading training/images/satImage_008.png\n",
      "Loading training/images/satImage_009.png\n",
      "Loading training/images/satImage_010.png\n",
      "Loading training/images/satImage_011.png\n",
      "Loading training/images/satImage_012.png\n",
      "Loading training/images/satImage_013.png\n",
      "Loading training/images/satImage_014.png\n",
      "Loading training/images/satImage_015.png\n",
      "Loading training/images/satImage_016.png\n",
      "Loading training/images/satImage_017.png\n",
      "Loading training/images/satImage_018.png\n",
      "Loading training/images/satImage_019.png\n",
      "Loading training/images/satImage_020.png\n",
      "Loading training/images/satImage_021.png\n",
      "Loading training/images/satImage_022.png\n",
      "Loading training/images/satImage_023.png\n",
      "Loading training/images/satImage_024.png\n",
      "Loading training/images/satImage_025.png\n",
      "Loading training/images/satImage_026.png\n",
      "Loading training/images/satImage_027.png\n",
      "Loading training/images/satImage_028.png\n",
      "Loading training/images/satImage_029.png\n",
      "Loading training/images/satImage_030.png\n",
      "Loading training/images/satImage_031.png\n",
      "Loading training/images/satImage_032.png\n",
      "Loading training/images/satImage_033.png\n",
      "Loading training/images/satImage_034.png\n",
      "Loading training/images/satImage_035.png\n",
      "Loading training/images/satImage_036.png\n",
      "Loading training/images/satImage_037.png\n",
      "Loading training/images/satImage_038.png\n",
      "Loading training/images/satImage_039.png\n",
      "Loading training/images/satImage_040.png\n",
      "(400, 400, 3)\n",
      "(40, 400, 400, 3)\n",
      "Loading training/groundtruth/satImage_001.png\n",
      "Loading training/groundtruth/satImage_002.png\n",
      "Loading training/groundtruth/satImage_003.png\n",
      "Loading training/groundtruth/satImage_004.png\n",
      "Loading training/groundtruth/satImage_005.png\n",
      "Loading training/groundtruth/satImage_006.png\n",
      "Loading training/groundtruth/satImage_007.png\n",
      "Loading training/groundtruth/satImage_008.png\n",
      "Loading training/groundtruth/satImage_009.png\n",
      "Loading training/groundtruth/satImage_010.png\n",
      "Loading training/groundtruth/satImage_011.png\n",
      "Loading training/groundtruth/satImage_012.png\n",
      "Loading training/groundtruth/satImage_013.png\n",
      "Loading training/groundtruth/satImage_014.png\n",
      "Loading training/groundtruth/satImage_015.png\n",
      "Loading training/groundtruth/satImage_016.png\n",
      "Loading training/groundtruth/satImage_017.png\n",
      "Loading training/groundtruth/satImage_018.png\n",
      "Loading training/groundtruth/satImage_019.png\n",
      "Loading training/groundtruth/satImage_020.png\n",
      "Loading training/groundtruth/satImage_021.png\n",
      "Loading training/groundtruth/satImage_022.png\n",
      "Loading training/groundtruth/satImage_023.png\n",
      "Loading training/groundtruth/satImage_024.png\n",
      "Loading training/groundtruth/satImage_025.png\n",
      "Loading training/groundtruth/satImage_026.png\n",
      "Loading training/groundtruth/satImage_027.png\n",
      "Loading training/groundtruth/satImage_028.png\n",
      "Loading training/groundtruth/satImage_029.png\n",
      "Loading training/groundtruth/satImage_030.png\n",
      "Loading training/groundtruth/satImage_031.png\n",
      "Loading training/groundtruth/satImage_032.png\n",
      "Loading training/groundtruth/satImage_033.png\n",
      "Loading training/groundtruth/satImage_034.png\n",
      "Loading training/groundtruth/satImage_035.png\n",
      "Loading training/groundtruth/satImage_036.png\n",
      "Loading training/groundtruth/satImage_037.png\n",
      "Loading training/groundtruth/satImage_038.png\n",
      "Loading training/groundtruth/satImage_039.png\n",
      "Loading training/groundtruth/satImage_040.png\n",
      "400 400\n",
      "(40, 400, 400, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "root_dir = \"training/\"\n",
    "image_dir = root_dir + \"images/\"\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "\n",
    "train_data = extract_data(image_dir, TRAINING_SIZE)\n",
    "\n",
    "print(train_data.shape)\n",
    "train_labels = extract_labels(gt_dir, TRAINING_SIZE)\n",
    "print(train_labels.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "#print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import Input, merge, Conv2D, Conv2DTranspose,MaxPooling2D,Activation, UpSampling2D, Dropout, Cropping2D, concatenate, Dense, Flatten\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "def load_image(infilename):\n",
    "    data = mpimg.imread(infilename)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Cnn:\n",
    "    \n",
    "    def __init__(self):   \n",
    "        self.window_size = 64\n",
    "        self.patch_size = 16\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        window_size = self.window_size\n",
    "        patch_size = self.patch_size\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv2D(32, (3, 3), input_shape=(16, 16, 3), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "        self.model.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        self.model.add(Conv2D(128, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(2, activation='sigmoid'))\n",
    "        \n",
    "        \n",
    "    def train(self, data, labels):\n",
    "   \n",
    "        window_size = self.window_size\n",
    "        patch_size = self.patch_size\n",
    "        batch_size = 128\n",
    "        num_classes = 2\n",
    "        #data, x_test, labels, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "        # To use the window sliding method we need to use padding to work on the corners\n",
    "        \n",
    "        samples_per_epoch = data.shape[0]*data.shape[1]*data.shape[2]//256\n",
    "        \n",
    "        pad_val = int((window_size-patch_size)/2)  \n",
    "        \n",
    "        X = np.zeros( (data.shape[0],data.shape[1] + 2*pad_val, data.shape[2] + 2*pad_val, data.shape[3]) )\n",
    "        Y = np.zeros( (labels.shape[0], labels.shape[1] + 2*pad_val, labels.shape[2] +2*pad_val) )\n",
    "        \n",
    "        for i in range(data.shape[0]):\n",
    "            X[i] = np.lib.pad(data[i], ((pad_val, pad_val), (pad_val, pad_val), (0, 0)), 'symmetric')\n",
    "            Y[i] = np.lib.pad(labels[i], ((pad_val, pad_val), (pad_val, pad_val)), 'symmetric')\n",
    "            \n",
    "\n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=Adam(lr=0.001),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        #def value_to_class(v):\n",
    "        #    foreground_threshold = 0.25\n",
    "        #    df = np.mean(v)\n",
    "        #    if df > foreground_threshold:\n",
    "        #        return 1\n",
    "        #    else:\n",
    "        #        return 0\n",
    "            \n",
    "        def generator():\n",
    "            \n",
    "            while True:\n",
    "                batch_features = numpy.zeros((batch_size, 64, 64, 3))\n",
    "                batch_labels = numpy.zeros(batch_size)\n",
    "                for i in range(batch_size):\n",
    "                    idx = numpy.random.choice(data.shape[0])\n",
    "                    shape = data[idx].shape\n",
    "                    wnd_center = numpy.random.randint(window_size/2, shape[0]-window_size/2,2)\n",
    "                    \n",
    "                    im_patch = data[idx][wnd_center[0] - int(window_size/2): wnd_center[0] + int(window_size/2),\n",
    "                                              wnd_center[1] - int(window_size/2): wnd_center[1] + int(window_size/2)]\n",
    "                    gt_patch = labels[idx][wnd_center[0] - int(patch_size/2): wnd_center[0] + int(window_size/2),\n",
    "                                              wnd_center[1] - int(patch_size/2): wnd_center[1] + int(window_size/2)]\n",
    "                    \n",
    "                    threshold = 0.25\n",
    "                    label = (numpy.array([numpy.mean(gt_patch)]) > threshold) * 1\n",
    "                    \n",
    "                    batch_features[i] = im_patch\n",
    "                    batch_labels[i] =  label #value_to_class(np.array(gt_patch)) #labels[idx]\n",
    "                    \n",
    "                    \n",
    "            yield (batch_features, batch_labels)\n",
    "\n",
    "      \n",
    "\n",
    "        #self.model.fit_generator(generator(),validation_data=generator(data,labels),validation_steps=0.1,\n",
    "                        #samples_per_epoch=1000,\n",
    "                        #   nb_epoch=20,\n",
    "                        #   verbose=1)\n",
    "\n",
    "        self.model.fit_generator(generator(),\n",
    "                            steps_per_epoch=samples_per_epoch/batch_size,\n",
    "                            nb_epoch=20,\n",
    "                            verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_40 (Conv2D)           (None, 16, 16, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 94,274\n",
      "Trainable params: 94,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Cnn()\n",
    "model.build_model()\n",
    "print(model.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 5 images\n",
      "Loading 5 images\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load the training set\n",
    "root_dir = \"training/\"\n",
    "\n",
    "image_dir = root_dir + \"images/\"\n",
    "files = os.listdir(image_dir)\n",
    "n = 5 #len(files)\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = np.asarray([load_image(image_dir + files[i]) for i in range(n)])\n",
    "\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "gt_imgs = np.asarray([load_image(gt_dir + files[i]) for i in range(n)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inesbahej/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:115: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/Users/inesbahej/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:115: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., steps_per_epoch=24.4140625, verbose=1, epochs=20)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "model.train(imgs, gt_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
