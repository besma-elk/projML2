{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/inesbahej/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import matplotlib.image as mpimg\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 3 # RGB images\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 2\n",
    "TRAINING_SIZE = 40\n",
    "VALIDATION_SIZE = 5  # Size of the validation set.\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "BATCH_SIZE = 16 # 64\n",
    "NUM_EPOCHS = 50 #5\n",
    "RESTORE_MODEL = False # If True, restore existing model instead of training a new one\n",
    "RECORDING_STEP = 1000\n",
    "IMG_PATCH_SIZE = 1\n",
    "\n",
    "def load_image(infilename):\n",
    "    data = mpimg.imread(infilename)\n",
    "    return data\n",
    "\n",
    "def img_float_to_uint8(img):\n",
    "    rimg = img - numpy.min(img)\n",
    "    rimg = (rimg / numpy.max(rimg) * PIXEL_DEPTH).round().astype(numpy.uint8)\n",
    "    return rimg\n",
    "\n",
    "def concatenate_images(img, gt_img):\n",
    "    nChannels = len(gt_img.shape)\n",
    "    w = gt_img.shape[0]\n",
    "    h = gt_img.shape[1]\n",
    "    if nChannels == 3:\n",
    "        cimg = numpy.concatenate((img, gt_img), axis=1)\n",
    "    else:\n",
    "        gt_img_3c = numpy.zeros((w, h, 3), dtype=numpy.uint8)\n",
    "        gt_img8 = img_float_to_uint8(gt_img)          \n",
    "        gt_img_3c[:,:,0] = gt_img8\n",
    "        gt_img_3c[:,:,1] = gt_img8\n",
    "        gt_img_3c[:,:,2] = gt_img8\n",
    "        img8 = img_float_to_uint8(img)\n",
    "        cimg = numpy.concatenate((img8, gt_img_3c), axis=1)\n",
    "    return cimg\n",
    "\n",
    "\n",
    "def img_crop(im, w, h):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if is_2d:\n",
    "                im_patch = im[j:j+w, i:i+h]\n",
    "            else:\n",
    "                im_patch = im[j:j+w, i:i+h, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches\n",
    "\n",
    "foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "\n",
    "\n",
    "    \n",
    "def extract_labels(filename, num_images):\n",
    "    \"\"\"Extract the labels into a 1-hot matrix [image index, label index].\"\"\"\n",
    "    gt_imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            gt_imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    num_images = len(gt_imgs)\n",
    "    #print(gt_imgs[0].shape)\n",
    "    gt_exp = numpy.expand_dims(gt_imgs, axis=3)\n",
    "    #print(tmp.shape)\n",
    "    IMG_WIDTH = gt_exp[0].shape[0]\n",
    "    IMG_HEIGHT = gt_exp[0].shape[1]\n",
    "    print(IMG_WIDTH, IMG_HEIGHT)\n",
    "    gt_patches = [img_crop(gt_exp[i], IMG_WIDTH, IMG_HEIGHT) for i in range(num_images)]\n",
    "    data = [gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))]\n",
    "    #labels = numpy.asarray([value_to_class(numpy.mean(data[i])) for i in range(len(data))])\n",
    "\n",
    "    # Convert to dense 1-hot representation.\n",
    "    #return labels.astype(numpy.float32)\n",
    "    return numpy.asarray(data)\n",
    "\n",
    "\n",
    "def extract_data(filename, num_images):\n",
    "    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    num_images = len(imgs)\n",
    "    print(imgs[0].shape)\n",
    "    IMG_WIDTH = imgs[0].shape[0]\n",
    "    IMG_HEIGHT = imgs[0].shape[1]\n",
    "    #N_PATCHES_PER_IMAGE = (IMG_WIDTH/IMG_PATCH_SIZE)*(IMG_HEIGHT/IMG_PATCH_SIZE)\n",
    "\n",
    "    img_patches = [img_crop(imgs[i], IMG_WIDTH, IMG_HEIGHT) for i in range(num_images)]\n",
    "    data = [img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))]\n",
    "\n",
    "    return numpy.asarray(data)\n",
    "    #return imgs\n",
    "    \n",
    "    \n",
    "\n",
    "def value_to_class(v):\n",
    "    foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "    df = numpy.sum(v)\n",
    "    if df > foreground_threshold:\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        return [1, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training/images/satImage_001.png\n",
      "Loading training/images/satImage_002.png\n",
      "Loading training/images/satImage_003.png\n",
      "Loading training/images/satImage_004.png\n",
      "Loading training/images/satImage_005.png\n",
      "Loading training/images/satImage_006.png\n",
      "Loading training/images/satImage_007.png\n",
      "Loading training/images/satImage_008.png\n",
      "Loading training/images/satImage_009.png\n",
      "Loading training/images/satImage_010.png\n",
      "Loading training/images/satImage_011.png\n",
      "Loading training/images/satImage_012.png\n",
      "Loading training/images/satImage_013.png\n",
      "Loading training/images/satImage_014.png\n",
      "Loading training/images/satImage_015.png\n",
      "Loading training/images/satImage_016.png\n",
      "Loading training/images/satImage_017.png\n",
      "Loading training/images/satImage_018.png\n",
      "Loading training/images/satImage_019.png\n",
      "Loading training/images/satImage_020.png\n",
      "Loading training/images/satImage_021.png\n",
      "Loading training/images/satImage_022.png\n",
      "Loading training/images/satImage_023.png\n",
      "Loading training/images/satImage_024.png\n",
      "Loading training/images/satImage_025.png\n",
      "Loading training/images/satImage_026.png\n",
      "Loading training/images/satImage_027.png\n",
      "Loading training/images/satImage_028.png\n",
      "Loading training/images/satImage_029.png\n",
      "Loading training/images/satImage_030.png\n",
      "Loading training/images/satImage_031.png\n",
      "Loading training/images/satImage_032.png\n",
      "Loading training/images/satImage_033.png\n",
      "Loading training/images/satImage_034.png\n",
      "Loading training/images/satImage_035.png\n",
      "Loading training/images/satImage_036.png\n",
      "Loading training/images/satImage_037.png\n",
      "Loading training/images/satImage_038.png\n",
      "Loading training/images/satImage_039.png\n",
      "Loading training/images/satImage_040.png\n",
      "(400, 400, 3)\n",
      "(40, 400, 400, 3)\n",
      "Loading training/groundtruth/satImage_001.png\n",
      "Loading training/groundtruth/satImage_002.png\n",
      "Loading training/groundtruth/satImage_003.png\n",
      "Loading training/groundtruth/satImage_004.png\n",
      "Loading training/groundtruth/satImage_005.png\n",
      "Loading training/groundtruth/satImage_006.png\n",
      "Loading training/groundtruth/satImage_007.png\n",
      "Loading training/groundtruth/satImage_008.png\n",
      "Loading training/groundtruth/satImage_009.png\n",
      "Loading training/groundtruth/satImage_010.png\n",
      "Loading training/groundtruth/satImage_011.png\n",
      "Loading training/groundtruth/satImage_012.png\n",
      "Loading training/groundtruth/satImage_013.png\n",
      "Loading training/groundtruth/satImage_014.png\n",
      "Loading training/groundtruth/satImage_015.png\n",
      "Loading training/groundtruth/satImage_016.png\n",
      "Loading training/groundtruth/satImage_017.png\n",
      "Loading training/groundtruth/satImage_018.png\n",
      "Loading training/groundtruth/satImage_019.png\n",
      "Loading training/groundtruth/satImage_020.png\n",
      "Loading training/groundtruth/satImage_021.png\n",
      "Loading training/groundtruth/satImage_022.png\n",
      "Loading training/groundtruth/satImage_023.png\n",
      "Loading training/groundtruth/satImage_024.png\n",
      "Loading training/groundtruth/satImage_025.png\n",
      "Loading training/groundtruth/satImage_026.png\n",
      "Loading training/groundtruth/satImage_027.png\n",
      "Loading training/groundtruth/satImage_028.png\n",
      "Loading training/groundtruth/satImage_029.png\n",
      "Loading training/groundtruth/satImage_030.png\n",
      "Loading training/groundtruth/satImage_031.png\n",
      "Loading training/groundtruth/satImage_032.png\n",
      "Loading training/groundtruth/satImage_033.png\n",
      "Loading training/groundtruth/satImage_034.png\n",
      "Loading training/groundtruth/satImage_035.png\n",
      "Loading training/groundtruth/satImage_036.png\n",
      "Loading training/groundtruth/satImage_037.png\n",
      "Loading training/groundtruth/satImage_038.png\n",
      "Loading training/groundtruth/satImage_039.png\n",
      "Loading training/groundtruth/satImage_040.png\n",
      "400 400\n",
      "(40, 400, 400, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "root_dir = \"training/\"\n",
    "image_dir = root_dir + \"images/\"\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "\n",
    "train_data = extract_data(image_dir, TRAINING_SIZE)\n",
    "\n",
    "print(train_data.shape)\n",
    "train_labels = extract_labels(gt_dir, TRAINING_SIZE)\n",
    "print(train_labels.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "#print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import Input, merge, Conv2D, Conv2DTranspose,MaxPooling2D,Activation, UpSampling2D, Dropout, Cropping2D, concatenate, Dense, Flatten\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "from keras.constraints import maxnorm\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def load_image(infilename):\n",
    "    data = mpimg.imread(infilename)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Cnn:\n",
    "    \n",
    "    def __init__(self):   \n",
    "        self.window_size = 64\n",
    "        self.patch_size = 16\n",
    "        self.pad_val = int((self.window_size-self.patch_size)/2)  \n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        window_size = self.window_size\n",
    "        patch_size = self.patch_size\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv2D(32, (3, 3), input_shape=(window_size, window_size, 3), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "        self.model.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        self.model.add(Conv2D(128, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(2, activation='sigmoid'))\n",
    "        \n",
    "        \n",
    "    def train(self, data, labels):\n",
    "   \n",
    "        window_size = self.window_size\n",
    "        patch_size = self.patch_size\n",
    "        batch_size = 128\n",
    "        num_classes = 2\n",
    "\n",
    "        \n",
    "        samples_per_epoch = data.shape[0]*data.shape[1]*data.shape[2]//256\n",
    "\n",
    "        # To use the window sliding method we need to use padding to work on the corners\n",
    "        pad_val = int((window_size-patch_size)/2)  \n",
    "        \n",
    "        X = np.zeros( (data.shape[0],data.shape[1] + 2*pad_val, data.shape[2] + 2*pad_val, data.shape[3]) )\n",
    "        Y = np.zeros( (labels.shape[0], labels.shape[1] + 2*pad_val, labels.shape[2] +2*pad_val) )\n",
    "        \n",
    "        for i in range(data.shape[0]):\n",
    "            #X[i] = np.lib.pad(data[i], ((pad_val, pad_val), (pad_val, pad_val), (0, 0)), 'symmetric')\n",
    "            #Y[i] = np.lib.pad(labels[i], ((pad_val, pad_val), (pad_val, pad_val)), 'symmetric')\n",
    "            X[i] = np.lib.pad(data[i], ((pad_val, pad_val), (pad_val, pad_val), (0, 0)), 'reflect')\n",
    "            Y[i] = np.lib.pad(labels[i], ((pad_val, pad_val), (pad_val, pad_val)), 'reflect')\n",
    "        data = X\n",
    "        labels = Y\n",
    "            \n",
    "\n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=Adam(lr=0.001),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        def value_to_class(v):\n",
    "            foreground_threshold = 0.25\n",
    "            df = np.array(v)\n",
    "            if df > foreground_threshold:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "        def gen():\n",
    "            \n",
    "            while True:\n",
    "                batch_features = numpy.zeros((batch_size, window_size, window_size, 3))\n",
    "                batch_labels = numpy.zeros((batch_size,2))\n",
    "                for i in range(batch_size):\n",
    "                    idx = numpy.random.choice(data.shape[0])\n",
    "                    \n",
    "                    wnd_center = numpy.random.randint(int(window_size/2), data[idx].shape[0]-int(window_size/2),2)\n",
    "                    \n",
    "                    batch_features[i] = data[idx][wnd_center[0] - int(window_size/2): wnd_center[0] + int(window_size/2),\n",
    "                                                  wnd_center[1] - int(window_size/2): wnd_center[1] + int(window_size/2)]\n",
    "                    \n",
    "                    gt_patch = labels[idx][wnd_center[0] - int(patch_size/2): wnd_center[0] + int(window_size/2),\n",
    "                                              wnd_center[1] - int(patch_size/2): wnd_center[1] + int(window_size/2)]\n",
    "                    \n",
    "                    label = value_to_class(numpy.mean(gt_patch))\n",
    "                    batch_labels[i] =  to_categorical(label, 2) \n",
    "                    \n",
    "                    \n",
    "                yield (batch_features, batch_labels)\n",
    "\n",
    "      \n",
    "\n",
    "        #self.model.fit_generator(generator(),validation_data=generator(data,labels),validation_steps=0.1,\n",
    "                        #samples_per_epoch=1000,\n",
    "                        #   nb_epoch=20,\n",
    "                        #   verbose=1)\n",
    "\n",
    "        self.model.fit_generator(gen(),\n",
    "                            steps_per_epoch=samples_per_epoch//batch_size,\n",
    "                            nb_epoch=20,\n",
    "                            verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 16386     \n",
      "=================================================================\n",
      "Total params: 109,634\n",
      "Trainable params: 109,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Cnn()\n",
    "model.build_model()\n",
    "print(model.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 images\n",
      "Loading 20 images\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load the training set\n",
    "root_dir = \"training/\"\n",
    "\n",
    "image_dir = root_dir + \"images/\"\n",
    "files = os.listdir(image_dir)\n",
    "n = 20 #len(files)\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = np.asarray([load_image(image_dir + files[i]) for i in range(n)])\n",
    "\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "gt_imgs = np.asarray([load_image(gt_dir + files[i]) for i in range(n)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inesbahej/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:122: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/Users/inesbahej/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:122: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., steps_per_epoch=97, verbose=1, epochs=20)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "97/97 [==============================] - 131s 1s/step - loss: 0.5861 - acc: 0.6876\n",
      "Epoch 2/20\n",
      "97/97 [==============================] - 144s 1s/step - loss: 0.5117 - acc: 0.7261\n",
      "Epoch 3/20\n",
      "97/97 [==============================] - 126s 1s/step - loss: 0.4602 - acc: 0.7717\n",
      "Epoch 4/20\n",
      "97/97 [==============================] - 129s 1s/step - loss: 0.4204 - acc: 0.7970\n",
      "Epoch 5/20\n",
      "97/97 [==============================] - 128s 1s/step - loss: 0.3794 - acc: 0.8270\n",
      "Epoch 6/20\n",
      "97/97 [==============================] - 131s 1s/step - loss: 0.3356 - acc: 0.8484\n",
      "Epoch 7/20\n",
      "97/97 [==============================] - 126s 1s/step - loss: 0.3175 - acc: 0.8610\n",
      "Epoch 8/20\n",
      "97/97 [==============================] - 127s 1s/step - loss: 0.2789 - acc: 0.8783\n",
      "Epoch 9/20\n",
      "97/97 [==============================] - 126s 1s/step - loss: 0.2693 - acc: 0.8842\n",
      "Epoch 10/20\n",
      "97/97 [==============================] - 127s 1s/step - loss: 0.2481 - acc: 0.8972\n",
      "Epoch 11/20\n",
      "97/97 [==============================] - 127s 1s/step - loss: 0.2253 - acc: 0.9035\n",
      "Epoch 12/20\n",
      "97/97 [==============================] - 149s 2s/step - loss: 0.2148 - acc: 0.9116\n",
      "Epoch 13/20\n",
      "97/97 [==============================] - 144s 1s/step - loss: 0.2073 - acc: 0.9147\n",
      "Epoch 14/20\n",
      "97/97 [==============================] - 141s 1s/step - loss: 0.2077 - acc: 0.9133\n",
      "Epoch 15/20\n",
      "97/97 [==============================] - 148s 2s/step - loss: 0.1768 - acc: 0.9271\n",
      "Epoch 16/20\n",
      "97/97 [==============================] - 144s 1s/step - loss: 0.1750 - acc: 0.9306\n",
      "Epoch 17/20\n",
      "97/97 [==============================] - 130s 1s/step - loss: 0.1648 - acc: 0.9325\n",
      "Epoch 18/20\n",
      "97/97 [==============================] - 141s 1s/step - loss: 0.1665 - acc: 0.9335\n",
      "Epoch 19/20\n",
      "97/97 [==============================] - 133s 1s/step - loss: 0.1646 - acc: 0.9369\n",
      "Epoch 20/20\n",
      "97/97 [==============================] - 127s 1s/step - loss: 0.1579 - acc: 0.9357\n"
     ]
    }
   ],
   "source": [
    "model.train(imgs, gt_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 50 images\n",
      "Loading 50 images\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_4_input to have shape (None, 64, 64, 3) but got array with shape (50, 400, 400, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-fb2a83cccea4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/inesbahej/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight)\u001b[0m\n\u001b[1;32m    987\u001b[0m                                    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m                                    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m                                    sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/inesbahej/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1723\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1724\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `_test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/inesbahej/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1412\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1415\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1416\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/inesbahej/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    151\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_4_input to have shape (None, 64, 64, 3) but got array with shape (50, 400, 400, 3)"
     ]
    }
   ],
   "source": [
    "root_dir = \"training/\"\n",
    "\n",
    "image_dir = root_dir + \"images/\"\n",
    "files = os.listdir(image_dir)\n",
    "n =  50 #len(files)\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "X_test = np.asarray([load_image(image_dir + files[i]) for i in range(50,len(files))])\n",
    "\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "y_test = np.asarray([load_image(gt_dir + files[i]) for i in range(50,len(files))])\n",
    "\n",
    "scores = model.model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
