{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN (U-NET Below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import matplotlib.image as mpimg\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "from PIL import Image\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image(infilename):\n",
    "    data = mpimg.imread(infilename)\n",
    "    return data\n",
    "\n",
    "def img_float_to_uint8(img):\n",
    "    rimg = img - numpy.min(img)\n",
    "    rimg = (rimg / numpy.max(rimg) * PIXEL_DEPTH).round().astype(numpy.uint8)\n",
    "    return rimg\n",
    "\n",
    "def concatenate_images(img, gt_img):\n",
    "    nChannels = len(gt_img.shape)\n",
    "    w = gt_img.shape[0]\n",
    "    h = gt_img.shape[1]\n",
    "    if nChannels == 3:\n",
    "        cimg = numpy.concatenate((img, gt_img), axis=1)\n",
    "    else:\n",
    "        gt_img_3c = numpy.zeros((w, h, 3), dtype=numpy.uint8)\n",
    "        gt_img8 = img_float_to_uint8(gt_img)          \n",
    "        gt_img_3c[:,:,0] = gt_img8\n",
    "        gt_img_3c[:,:,1] = gt_img8\n",
    "        gt_img_3c[:,:,2] = gt_img8\n",
    "        img8 = img_float_to_uint8(img)\n",
    "        cimg = numpy.concatenate((img8, gt_img_3c), axis=1)\n",
    "    return cimg\n",
    "\n",
    "def extract_labels(filename, num_images):\n",
    "    \"\"\"Extract the labels into a 1-hot matrix [image index, label index].\"\"\"\n",
    "    gt_imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            gt_imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    num_images = len(gt_imgs)\n",
    "    gt_patches = [img_crop(gt_imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE) for i in range(num_images)]\n",
    "    data = numpy.asarray([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])\n",
    "    labels = numpy.asarray([value_to_class(numpy.mean(data[i])) for i in range(len(data))])\n",
    "\n",
    "    # Convert to dense 1-hot representation.\n",
    "    return labels.astype(numpy.float32)\n",
    "\n",
    "def img_crop(im, w, h):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if is_2d:\n",
    "                im_patch = im[j:j+w, i:i+h]\n",
    "            else:\n",
    "                im_patch = im[j:j+w, i:i+h, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches\n",
    "\n",
    "foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "\n",
    "def value_to_class(v):\n",
    "    foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "    df = numpy.sum(v)\n",
    "    if df > foreground_threshold:\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        return [1, 0]\n",
    "        \n",
    "def extract_data(filename, num_images):\n",
    "    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    num_images = len(imgs)\n",
    "    IMG_WIDTH = imgs[0].shape[0]\n",
    "    IMG_HEIGHT = imgs[0].shape[1]\n",
    "    N_PATCHES_PER_IMAGE = (IMG_WIDTH/IMG_PATCH_SIZE)*(IMG_HEIGHT/IMG_PATCH_SIZE)\n",
    "\n",
    "    img_patches = [img_crop(imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE) for i in range(num_images)]\n",
    "    data = [img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))]\n",
    "\n",
    "    return numpy.asarray(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training/images/satImage_001.png\n",
      "Loading training/images/satImage_002.png\n",
      "Loading training/images/satImage_003.png\n",
      "Loading training/images/satImage_004.png\n",
      "Loading training/images/satImage_005.png\n",
      "Loading training/images/satImage_006.png\n",
      "Loading training/images/satImage_007.png\n",
      "Loading training/images/satImage_008.png\n",
      "Loading training/images/satImage_009.png\n",
      "Loading training/images/satImage_010.png\n",
      "Loading training/images/satImage_011.png\n",
      "Loading training/images/satImage_012.png\n",
      "Loading training/images/satImage_013.png\n",
      "Loading training/images/satImage_014.png\n",
      "Loading training/images/satImage_015.png\n",
      "Loading training/images/satImage_016.png\n",
      "Loading training/images/satImage_017.png\n",
      "Loading training/images/satImage_018.png\n",
      "Loading training/images/satImage_019.png\n",
      "Loading training/images/satImage_020.png\n",
      "Loading training/groundtruth/satImage_001.png\n",
      "Loading training/groundtruth/satImage_002.png\n",
      "Loading training/groundtruth/satImage_003.png\n",
      "Loading training/groundtruth/satImage_004.png\n",
      "Loading training/groundtruth/satImage_005.png\n",
      "Loading training/groundtruth/satImage_006.png\n",
      "Loading training/groundtruth/satImage_007.png\n",
      "Loading training/groundtruth/satImage_008.png\n",
      "Loading training/groundtruth/satImage_009.png\n",
      "Loading training/groundtruth/satImage_010.png\n",
      "Loading training/groundtruth/satImage_011.png\n",
      "Loading training/groundtruth/satImage_012.png\n",
      "Loading training/groundtruth/satImage_013.png\n",
      "Loading training/groundtruth/satImage_014.png\n",
      "Loading training/groundtruth/satImage_015.png\n",
      "Loading training/groundtruth/satImage_016.png\n",
      "Loading training/groundtruth/satImage_017.png\n",
      "Loading training/groundtruth/satImage_018.png\n",
      "Loading training/groundtruth/satImage_019.png\n",
      "Loading training/groundtruth/satImage_020.png\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "NUM_CHANNELS = 3 # RGB images\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 2\n",
    "TRAINING_SIZE = 20\n",
    "VALIDATION_SIZE = 5  # Size of the validation set.\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "BATCH_SIZE = 16 # 64\n",
    "NUM_EPOCHS = 5\n",
    "RESTORE_MODEL = False # If True, restore existing model instead of training a new one\n",
    "RECORDING_STEP = 1000\n",
    "\n",
    "# Set image patch size in pixels\n",
    "# IMG_PATCH_SIZE should be a multiple of 4\n",
    "# image size should be an integer multiple of this number!\n",
    "IMG_PATCH_SIZE = 16\n",
    "\n",
    "root_dir = \"training/\"\n",
    "image_dir = root_dir + \"images/\"\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "\n",
    "train_data = extract_data(image_dir, TRAINING_SIZE)\n",
    "train_labels = extract_labels(gt_dir, TRAINING_SIZE)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 512)         147968    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 162,210\n",
      "Trainable params: 162,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "    \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(16, 16,3), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(32, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "#model.add(Activation('sigmoid'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "epochs = NUM_EPOCHS\n",
    "lrate = 0.01\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=1e-6, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 2500 samples\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 14s 1ms/step - loss: 0.5494 - acc: 0.7558 - val_loss: 0.5362 - val_acc: 0.7504\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 14s 1ms/step - loss: 0.5216 - acc: 0.7574 - val_loss: 0.5174 - val_acc: 0.7504\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 14s 1ms/step - loss: 0.5112 - acc: 0.7574 - val_loss: 0.5392 - val_acc: 0.7504\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 14s 1ms/step - loss: 0.5089 - acc: 0.7574 - val_loss: 0.5168 - val_acc: 0.7504\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 0.4972 - acc: 0.7575 - val_loss: 0.5035 - val_acc: 0.7504\n",
      "Accuracy: 75.04%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=16)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import matplotlib.image as mpimg\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "from PIL import Image\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image(infilename):\n",
    "    data = mpimg.imread(infilename)\n",
    "    return data\n",
    "\n",
    "def img_float_to_uint8(img):\n",
    "    rimg = img - numpy.min(img)\n",
    "    rimg = (rimg / numpy.max(rimg) * PIXEL_DEPTH).round().astype(numpy.uint8)\n",
    "    return rimg\n",
    "\n",
    "def concatenate_images(img, gt_img):\n",
    "    nChannels = len(gt_img.shape)\n",
    "    w = gt_img.shape[0]\n",
    "    h = gt_img.shape[1]\n",
    "    if nChannels == 3:\n",
    "        cimg = numpy.concatenate((img, gt_img), axis=1)\n",
    "    else:\n",
    "        gt_img_3c = numpy.zeros((w, h, 3), dtype=numpy.uint8)\n",
    "        gt_img8 = img_float_to_uint8(gt_img)          \n",
    "        gt_img_3c[:,:,0] = gt_img8\n",
    "        gt_img_3c[:,:,1] = gt_img8\n",
    "        gt_img_3c[:,:,2] = gt_img8\n",
    "        img8 = img_float_to_uint8(img)\n",
    "        cimg = numpy.concatenate((img8, gt_img_3c), axis=1)\n",
    "    return cimg\n",
    "\n",
    "def extract_labels(filename, num_images):\n",
    "    \"\"\"Extract the labels into a 1-hot matrix [image index, label index].\"\"\"\n",
    "    gt_imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            gt_imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    num_images = len(gt_imgs)\n",
    "    gt_patches = [img_crop(gt_imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE) for i in range(num_images)]\n",
    "    data = numpy.asarray([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])\n",
    "    labels = numpy.asarray([value_to_class(numpy.mean(data[i])) for i in range(len(data))])\n",
    "\n",
    "    # Convert to dense 1-hot representation.\n",
    "    return labels.astype(numpy.float32)\n",
    "\n",
    "def img_crop(im, w, h):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if is_2d:\n",
    "                im_patch = im[j:j+w, i:i+h]\n",
    "            else:\n",
    "                im_patch = im[j:j+w, i:i+h, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches\n",
    "\n",
    "foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "\n",
    "def value_to_class(v):\n",
    "    foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "    df = numpy.sum(v)\n",
    "    if df > foreground_threshold:\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        return [1, 0]\n",
    "        \n",
    "def extract_data(filename, num_images):\n",
    "    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    num_images = len(imgs)\n",
    "    IMG_WIDTH = imgs[0].shape[0]\n",
    "    IMG_HEIGHT = imgs[0].shape[1]\n",
    "    N_PATCHES_PER_IMAGE = (IMG_WIDTH/IMG_PATCH_SIZE)*(IMG_HEIGHT/IMG_PATCH_SIZE)\n",
    "\n",
    "    img_patches = [img_crop(imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE) for i in range(num_images)]\n",
    "    data = [img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))]\n",
    "\n",
    "    return numpy.asarray(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training/images/satImage_001.png\n",
      "Loading training/images/satImage_002.png\n",
      "Loading training/images/satImage_003.png\n",
      "Loading training/images/satImage_004.png\n",
      "Loading training/images/satImage_005.png\n",
      "Loading training/images/satImage_006.png\n",
      "Loading training/images/satImage_007.png\n",
      "Loading training/images/satImage_008.png\n",
      "Loading training/images/satImage_009.png\n",
      "Loading training/images/satImage_010.png\n",
      "Loading training/images/satImage_011.png\n",
      "Loading training/images/satImage_012.png\n",
      "Loading training/images/satImage_013.png\n",
      "Loading training/images/satImage_014.png\n",
      "Loading training/images/satImage_015.png\n",
      "Loading training/images/satImage_016.png\n",
      "Loading training/images/satImage_017.png\n",
      "Loading training/images/satImage_018.png\n",
      "Loading training/images/satImage_019.png\n",
      "Loading training/images/satImage_020.png\n",
      "Loading training/groundtruth/satImage_001.png\n",
      "Loading training/groundtruth/satImage_002.png\n",
      "Loading training/groundtruth/satImage_003.png\n",
      "Loading training/groundtruth/satImage_004.png\n",
      "Loading training/groundtruth/satImage_005.png\n",
      "Loading training/groundtruth/satImage_006.png\n",
      "Loading training/groundtruth/satImage_007.png\n",
      "Loading training/groundtruth/satImage_008.png\n",
      "Loading training/groundtruth/satImage_009.png\n",
      "Loading training/groundtruth/satImage_010.png\n",
      "Loading training/groundtruth/satImage_011.png\n",
      "Loading training/groundtruth/satImage_012.png\n",
      "Loading training/groundtruth/satImage_013.png\n",
      "Loading training/groundtruth/satImage_014.png\n",
      "Loading training/groundtruth/satImage_015.png\n",
      "Loading training/groundtruth/satImage_016.png\n",
      "Loading training/groundtruth/satImage_017.png\n",
      "Loading training/groundtruth/satImage_018.png\n",
      "Loading training/groundtruth/satImage_019.png\n",
      "Loading training/groundtruth/satImage_020.png\n",
      "(10000, 16, 16, 3) (2500, 16, 16, 3) (10000, 2) (2500, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "NUM_CHANNELS = 3 # RGB images\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 2\n",
    "TRAINING_SIZE = 20\n",
    "VALIDATION_SIZE = 5  # Size of the validation set.\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "BATCH_SIZE = 16 # 64\n",
    "NUM_EPOCHS = 5\n",
    "RESTORE_MODEL = False # If True, restore existing model instead of training a new one\n",
    "RECORDING_STEP = 1000\n",
    "IMG_PATCH_SIZE = 16\n",
    "\n",
    "root_dir = \"training/\"\n",
    "image_dir = root_dir + \"images/\"\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "\n",
    "train_data = extract_data(image_dir, TRAINING_SIZE)\n",
    "train_labels = extract_labels(gt_dir, TRAINING_SIZE)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 shape: (?, 16, 16, 64)\n",
      "conv1 shape: (?, 16, 16, 64)\n",
      "pool1 shape: (?, 8, 8, 64)\n",
      "conv2 shape: (?, 8, 8, 128)\n",
      "conv2 shape: (?, 8, 8, 128)\n",
      "pool2 shape: (?, 4, 4, 128)\n",
      "conv3 shape: (?, 4, 4, 256)\n",
      "conv3 shape: (?, 4, 4, 256)\n",
      "pool3 shape: (?, 2, 2, 256)\n",
      "(?, 2)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import Input, merge, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Cropping2D, concatenate, Dense, Flatten\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "\n",
    "\n",
    "input_x = Input(shape=(16, 16,3), name='input_x') \n",
    "conv1 = Conv2D(64, (3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(input_x)\n",
    "print(\"conv1 shape:\",conv1.shape)\n",
    "conv1 = Conv2D(64, (3,3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "print(\"conv1 shape:\",conv1.shape)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "print(\"pool1 shape:\",pool1.shape)\n",
    "\n",
    "conv2 = Conv2D(128, (3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "print(\"conv2 shape:\",conv2.shape)\n",
    "conv2 = Conv2D(128, (3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "print(\"conv2 shape:\",conv2.shape)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "print(\"pool2 shape:\",pool2.shape)\n",
    "\n",
    "conv3 = Conv2D(256, (3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "print(\"conv3 shape:\",conv3.shape)\n",
    "conv3 = Conv2D(256, (3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "print(\"conv3 shape:\",conv3.shape)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "print(\"pool3 shape:\",pool3.shape)\n",
    "\n",
    "conv4 = Conv2D(512, (3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "conv4 = Conv2D(512, (3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "drop4 = Dropout(0.5)(conv4)\n",
    "pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "conv5 = Conv2D(1024, (3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "conv5 = Conv2D(1024, (3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "#merge6 = merge([drop4,up6], mode = 'concat', concat_axis = 3)\n",
    "merge6 = concatenate([drop4,up6],axis = 3)\n",
    "conv6 = Conv2D(512, (3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "conv6 = Conv2D(512, (3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "#merge7 = merge([conv3,up7], mode = 'concat', concat_axis = 3)\n",
    "merge7 = concatenate([conv3,up7], axis = 3)\n",
    "conv7 = Conv2D(256, (3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "conv7 = Conv2D(256, (3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "#merge8 = merge([conv2,up8], mode = 'concat', concat_axis = 3)\n",
    "merge8 = concatenate([conv2,up8], axis = 3)\n",
    "conv8 = Conv2D(128, (3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "conv8 = Conv2D(128, (3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "#merge9 = merge([conv1,up9], mode = 'concat', concat_axis = 3)\n",
    "merge9 = concatenate([conv1,up9], axis = 3)\n",
    "conv9 = Conv2D(64, (3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "conv9 = Conv2D(64, (3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "conv9 = Conv2D(2, (3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "#conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "#print(conv10.shape)\n",
    "\n",
    "\n",
    "res = Dense(2, activation='sigmoid')(Flatten()(conv9))\n",
    "\n",
    "print(res.shape)\n",
    "model = Model(inputs = input_x, outputs = res) #conv10\n",
    "\n",
    "model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=16, epochs=10, verbose=1,validation_split=0.2, shuffle=True)\n",
    "#model.fit(X_train, y_train, batch_size=16, epochs=10, verbose=1,validation_data=(X_test, y_test), shuffle=True)\n",
    "#print('predict test data')\n",
    "#y_test = model.predict(X_test, batch_size=1, verbose=1)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
